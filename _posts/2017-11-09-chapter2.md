---
layout: post
title: "Chapter 2"
date: 2017-11-09 16:36:20
image: '/assets/img/'
description: 'Multi-Armed Bandits'
tags:
- Reinforcement Learning
- Reinforcement Learning-An Introduction 
categories:
- Reinforcement Learning-An Introduction 
twitter_text:
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

## Introduction

Reinforcement Learning is different from other machine learning in the aspect that it *evaluates* the actions rather than instructing than *instructing* the correct actions.
  1. Purely evaluative feedback indicates how good an action is , but not whether it is best or worst action possible.

  2. Purely instructive feedback indicates the correct action to take independent of the action already taken.

## K-Armed Bandit Problem

The k-armed bandit problem is similar to one-armed bandits, except that it has k levers instead of one. Here we are faced repeatedly with a choice among k different levers, or actions. Here the rewards are the payoffs for hitting the jackpot. 

In the problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action.We denote the action selected on time step t as $$A_t$$ , and the corresponding reward as $$R_t$$. The value then of an arbitrary action $$a$$, denoted $$q_∗(a)$$, is the expected reward given
that a is selected:

$$q_∗(a) = E[R_t | A_t = a]$$


We denote the estimated value of action $$a$$ at time $$t$$ as $$Q_t(a) \approx q_*(a)$$ .

Actions whose $$Q_t$$ value is the highest at time $$t$$, are called,  *greedy* actions.All the other actions at time $$t$$ are called *non-greedy* actions.Selecting a greedy action is said to be *exploitation* whereas selecting a non-greedy action is said to be *evaluation*.Exploitation is the right thing to do to maximize the expected on the one step, but exploration may produce greater total reward in the long run.

## Action Value Methods

So now we know that the true value of an action is the  mean reward when the action is chosen.There can be many ways of calcuate this, one way can be:

$$Q_t(a) = \frac{\text{sum of rewards when a is taken prior to t}}{\text{number of times a is taken prior to t}} = \frac{\sum_{i=0}^{t-1} R_i \cdot 1_{A_i = a}}{\sum_{i=0}^{t-1} 1_{A_i = a}}$$

where $$1_predicate$$ denotes the random variable that is $$1$$ if $$predicate$$ is true and $$0$$ if it is not. If the denominator is zero, then we instead denote $$Q_t(a)$$ as some default value, such as $$Q_1(a) = 0$$ . As the denominator goes to infnity, by the law of large numbers, $$Q_t(a)$$ converges to $$q_*(a)$$. We call this the *sample-average* method for estimating action values because each *estimate* is an average of the *sample* of relevant rewards.

The *greedy* actions selection method (a.k.a exploitation) can be represented as:

$$A_t = \underset{x}{\operatorname{argmax}}Q_t(a)$$

Where $$argmax_a$$ denotes the value of $$a$$ at which the expressio that follows is maximized(with ties broken arbitrarily). A simple alternative to this purely greedy approach is a *$$\varepsilon-greedy$$* approach where with probability $$\varepsilon$$ , select an action from the set of *non-greedy* actions unformly and randomly.

## Incremental Implementation

We would now dive into the implementation of the above formulae to reinforcement learning problems.
Lets concentrate on a single action $$a$$. Let $$R_i$$ denote the reward received after the $$ith$$ selection of this action.Let $$Q_n$$ denote the estimate of this action after it has been selected *n-1* times. Then we can write:

$$Q_n(a) = \frac{R_1 + R_2 +...+ R_{n-1}}{n-1}$$

Keeping a record of all the rewards will be inefficient in terms of memory, so we would tweak the formula a little bit:

$$\begin{align}
Q_{n+1} & = \frac{1}{n}\sum_{i=1}^{n} R_i \\
& = \frac{1}{n}\bigl(R_n + \sum_{i=1}^{n-1} R_i\bigr) \\
& = \frac{1}{n}\bigl(R_n + (n-1)\frac{1}{n-1}\sum_{i=1}^{n-1} R_i\bigr) \\
& = \frac{1}{n}\bigl(R_n + (n-1)Q_n\bigr) \\
& = \frac{1}{n}\bigl(R_n + nQ_n - Q_n\bigr) \\
& = Q_n + \frac{1}{n}[R_n - Q_n]
\end{align}$$

## A Simple Bandit Algorithm

$$\text{Initialize, for a} = 1 \text{to k}:$$
$$Q(a) \leftarrow 0$$
$$N(a) \leftarrow 0$$

$$\text{Repeat Forever:}$$
$$A \leftarrow \begin{function} 
                \underset{a}{\operatorname{argmax}}Q(a)  \text{with probability 1- \varepsilon (breaking ties randomly)} \\
                \text{ a random action with probability \varepsilon} \\
                \end{function}$$

$$R \leftarrow bandit(a)$$
$$N(A) \leftarrow N(A) + 1$$
$$Q(A) \leftarrow Q(A) + \frac{\frac{1}{N(A)}}{[R - Q(A)]}$$



## Work in Progress
## Please come back Soon

## Comments

Please feel free to comment in the comment section below