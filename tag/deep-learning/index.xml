<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning | Anurag Roy</title>
    <link>https://ranarag.github.io/tag/deep-learning/</link>
      <atom:link href="https://ranarag.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>deep learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 27 Jun 2020 19:45:20 +0530</lastBuildDate>
    <image>
      <url>https://ranarag.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>deep learning</title>
      <link>https://ranarag.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>MetaSegnet</title>
      <link>https://ranarag.github.io/post/metasegnet/</link>
      <pubDate>Sat, 27 Jun 2020 19:45:20 +0530</pubDate>
      <guid>https://ranarag.github.io/post/metasegnet/</guid>
      <description>&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;While Existing methods on few-shot image segmentation focus on 1-way segmentation, this paper focuses on k-way segmentation tasks.&lt;/p&gt;
&lt;h2 id=&#34;existing-few-shot-learning-algorithms-suffer-from&#34;&gt;Existing Few-shot learning algorithms suffer from:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Distribution Divergence:&lt;/strong&gt; Most existing methods require to be pre-trained on ImageNet. However, there can be cases where the segmentation task might be very different from ImageNet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hard to Extend:&lt;/strong&gt; Most existing models perform few-shot segmentation by utilizing metric learning methods to measure one-to-one similarity between train and test images in the 1-way setting. Such methods cannot be directly adapted to the K-way setting&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Difficult to train:&lt;/strong&gt; Existing methods employ complex embedding models which are hard to optimize.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-this-paper-proposes&#34;&gt;What this paper proposes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A new architecture called &lt;strong&gt;MetaSegNet&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;A Linear base learner&lt;/li&gt;
&lt;li&gt;The model is capable of performing K-way N-shot semantic segmentation&lt;/li&gt;
&lt;li&gt;The model does this by extracting global and local information&lt;/li&gt;
&lt;li&gt;The model  does not require any prior knowledge for performing the task&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-architecture&#34;&gt;Model Architecture:&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;metasegnet/image1.png&#34; alt=&#34;/static/metasegnet/image1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Figure showing the pipeline of MetaSegNet for 2-way semantic segmentation. The support set(Dtrain)  is used to train a linear classification model, and meta-loss is used to train an embedding model(meta learner), which can generalize across all tasks. The novel embedding model branches to extract local and global features for pixel-level classification.&lt;/p&gt;
&lt;h3 id=&#34;embedding-network&#34;&gt;Embedding Network&lt;/h3&gt;
&lt;p&gt;The embedding network consists of two submodules— the feature extractor and the feature fusion module. Feature extractor extracts the local and global features whiles the feature fusion module fuses those features to perform the segmentation task.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the feature extraction module, a ResNet-9 architecture is used with some minor changes. To handle the conflicting demands of multi-scale reasoning and full-resolution dense prediction, the max-pooling layers behind blocks 3 and 4 and use dilated convolution. Also an additional branch is added to extract global context. Two max pooling layers are added ahead and behind block-5 to expand the receptive field.&lt;/li&gt;
&lt;li&gt;In the feature fusion module, the global feature is first unpooled to the same size as the local feature map spatially and then concatenated together. Since the two types of features have different scales and norms, l2 norm is applied in each channel for normalization. After normalization, combined feature map is reshaped to pixel feature map for pixel-wise classification(i.e segmentation)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;differentiable-linear-base-learner&#34;&gt;Differentiable Linear Base Learner:&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why a linear base learner?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It is very difficult to calculate the analytic or optimal solution of the base learner if the base learner is a complex model with many non linearities in it. Such an assurance is required so as to ensure that the base learner is trained well, so as to train the meta learner well. Therefore, a differentiable linear model is chosen as the base learner.&lt;/p&gt;
&lt;p&gt;$$\Lambda = \underset{w}{\operatorname{argmin}}| Xw - y |^{2} + \lambda |w|^{2} $$&lt;/p&gt;
&lt;p&gt;where X is a pixel feature matrix obtained from the embedding network and y is the label for each pixel.&lt;/p&gt;
&lt;p&gt;The closed form solution for this is as follows:&lt;/p&gt;
&lt;p&gt;$$w = \big(X^TX +\lambda I \big)^{-1}X^Ty$$&lt;/p&gt;
&lt;p&gt;This closed-form solution makes it easy to backpropagate through the meta-learning loss which is defined as:&lt;/p&gt;
&lt;p&gt;$$\mathcal{L}^{meta}\left( \mathcal{D^{test}} ; w; \varphi, \lambda \right) = -\frac{1}{\left|\mathcal{T}\right|. \left|D_i^{test}\right|\left|\mathcal{I}\right|}\sum_{i \in \mathcal{T}}\sum_{(x,y) \in \mathcal{D_i^{test}}}\sum_{p \in \mathcal{I}}\log(S_{py_p})$$&lt;/p&gt;
&lt;p&gt;where I is the set of pixels in the image and let $S_{pc}$ is the score of pixel $p$ in class $c$, then  $S_{pc} = exp(s_{pc}) / \sum_{k=1}^{N} exp(s_{pk})$ represents the softmax probability of class $c$ at pixel $p$.&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Datasets used:&lt;/strong&gt; 2 datasets have been used — COCO and PASCAL VOC.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;metasegnet/res1.png&#34; alt=&#34;metasegnet/res1.png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;                        Result on COCO dataset
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;metasegnet/res2.png&#34; alt=&#34;metasegnet/res2.png&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;                        Result on PASCAL-5i dataset&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Retrieving Information from Multiple Sources</title>
      <link>https://ranarag.github.io/publication/www18/</link>
      <pubDate>Sat, 13 Jan 2018 17:36:22 +0530</pubDate>
      <guid>https://ranarag.github.io/publication/www18/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TensorBoard Basics</title>
      <link>https://ranarag.github.io/post/tensorboard_basics/</link>
      <pubDate>Wed, 08 Nov 2017 15:09:44 +0530</pubDate>
      <guid>https://ranarag.github.io/post/tensorboard_basics/</guid>
      <description>&lt;h2 id=&#34;what-is-tensorboard&#34;&gt;What is TensorBoard?&lt;/h2&gt;
&lt;p&gt;TensorBoard is a module of Tensorflow that provides a suite of visualisation tools that help in understanding, debugging and optimize the model created in Tensorflow.&lt;/p&gt;
&lt;h2 id=&#34;installing-tensorboard&#34;&gt;Installing TensorBoard&lt;/h2&gt;
&lt;p&gt;TensorBoard is installed along with Tensorflow.
It can also be installed as a standalone software from 
&lt;a href=&#34;https://github.com/dmlc/tensorboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Standalone TensorBoard&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-concepts&#34;&gt;Key Concepts&lt;/h2&gt;
&lt;p&gt;TensorBoard works by reading summary data from Tensorflow event files which is generated by tensorflow.&lt;/p&gt;
&lt;p&gt;Tensorflow creates a computational graph of the neural network model implemented.This model is then stored as a graph using the &lt;strong&gt;summary&lt;/strong&gt; method.The name field in the Tensorflow &lt;strong&gt;variables&lt;/strong&gt;, &lt;strong&gt;placeholders&lt;/strong&gt;, &lt;strong&gt;operations&lt;/strong&gt;, and &lt;strong&gt;name_scopes&lt;/strong&gt; are used to annotate the nodes in the graph. These graphs are written to files and stored in a directory indicated in the &lt;strong&gt;summary.FileWriter&lt;/strong&gt; method.We can select which nodes to be included in the graph using the &lt;strong&gt;summary.merge&lt;/strong&gt; method. In order to select all the nodes to be included in the graph by using the &lt;strong&gt;summary.merge_all&lt;/strong&gt; method.This summary (in the form of a protocol buffer) is added to the event file using the &lt;strong&gt;add_summary&lt;/strong&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
#Although the model in this code-segment will not converge, 
#this is just to show how tensorflow creates and saves summaries 
#in event files.
import tensorflow as tf
import numpy as np
N = 5
x = tf.placeholder(tf.float32, shape=[N,1],name=&#39;X&#39;) 
y = tf.placeholder(tf.float32, shape=[N,N],name=&#39;y&#39;) 
with tf.name_scope(&#39;fc_layer&#39;):
    W = tf.Variable(tf.truncated_normal([1,N], stddev=0.1), name=&#39;W&#39;)
    b = tf.Variable(tf.constant(0.1,shape=[N]),name=&#39;b&#39;)
    tf.summary.histogram(&#39;weights&#39;,W) #values of W will be shown as a histogram plot
    tf.summary.histogram(&#39;biases&#39;,b) #values of W will be shown as a histogram plot
    y_ = tf.matmul(x,W) + b
with tf.name_scope(&#39;loss_func&#39;):    
    
    tf.summary.scalar(&#39;loss&#39;,0.05)


sess = tf.Session()
merged_summary = tf.summary.merge_all()
writer = tf.summary.FileWriter(&amp;quot;./graphs/&amp;quot;, graph=sess.graph)
sess.run(tf.global_variables_initializer())
s = sess.run(merged_summary,feed_dict={x:np.ones((N,1)),y:np.ones((N,N))})
i = 1  
writer.add_summary(s,i)
```python

## Running TensorBoard

Now that the event files have been created and stored in the directory[./graphs], running tensorboard with logdir pointing to the directory where the event files have been stored will give us a visualization of the model in the web browser.
```python
tensorboard --logdir=./graphs
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Tensorflow Basics</title>
      <link>https://ranarag.github.io/post/tensorflow_basics/</link>
      <pubDate>Wed, 05 Jul 2017 21:25:58 +0530</pubDate>
      <guid>https://ranarag.github.io/post/tensorflow_basics/</guid>
      <description>&lt;h1 id=&#34;what-is-tensorflow&#34;&gt;What is Tensorflow?&lt;/h1&gt;
&lt;p&gt;Tensorflow is an open source software for machine learning developed by Google.Tensorflow, as one can get from its name, mainly handles matrices(or tensors), its mathematical operations and differentiation efficiently.It is just like Theano but with some extra features like  it can be used on distributed systems.&lt;/p&gt;
&lt;h2 id=&#34;installing-tensorflow&#34;&gt;Installing Tensorflow&lt;/h2&gt;
&lt;p&gt;Installing tensorflow &amp;gt;= 1.1.0 can be done using python-pip&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade tensorflow # for the CPU only version
pip install --upgrade tensorflow-gpu # for the GPU version
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;key-components&#34;&gt;Key-Components&lt;/h2&gt;
&lt;p&gt;Tensorflow works by first creating a computational graph resembling the model we wish to run and then executing it.
A program written using tensorflow must consist of the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Variables&lt;/strong&gt;: variables in tensorflow are in memory buffers containing tensors,but unlike normal tensors that live only for a single execution of a graph, variables values live as long as the session exists.Variables value cease to exist after the session is closed. Tensorflow has the option of saving variables&amp;rsquo; value to disk and restoring them for later use.Variables must be initialized before executing a graph for first time.
During training operation variables get updated by default.Tensorflow variables are created using &lt;strong&gt;tf.Variable()&lt;/strong&gt;.We can keep a variable unchanged by explicitly setting its trainable parameter to false.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
&amp;gt;&amp;gt;&amp;gt; # Declaring a tensoflow Variable
&amp;gt;&amp;gt;&amp;gt; W = tf.Variable(tf.random_uniform([2,3],stddev=0.5), # same a numpy.random.uniform
name=&#39;weight&#39;) # name of the variable in the computational graph
&amp;gt;&amp;gt;&amp;gt; # Setting variable b to non -trainable
&amp;gt;&amp;gt;&amp;gt; b = tf.Variable(tf.zeros([1]), # same as numpy.zeros
name=&#39;b&#39;, trainable=False)
&amp;gt;&amp;gt;&amp;gt; s = tf.Session() # session created
&amp;gt;&amp;gt;&amp;gt; s.run(tf.initialize_all_variables()) # To initialize all the variables present in the current session
&amp;gt;&amp;gt;&amp;gt; s.run(tf.initialize_variables(W)) # To only initialize W and not b
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Placeholders&lt;/strong&gt;: A placeholder can be thought of as a variable to which we can assign data at a later step.It is populated every single time a computational graph is run.A placeholder usually holds the input values to a model.Placeholder is created in tensorflow using &lt;strong&gt;tf.placeholder()&lt;/strong&gt;.Values in placeholders are entered using a feed_dict argument.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
&amp;gt;&amp;gt;&amp;gt; # Declaring a tensoflow Variable
&amp;gt;&amp;gt;&amp;gt; W = tf.Variable(tf.random_uniform([2,3],stddev=0.5), # same a numpy.random.uniform
name=&#39;weight&#39;) # name of the variable in the computational graph
&amp;gt;&amp;gt;&amp;gt; # Setting variable b to non -trainable
&amp;gt;&amp;gt;&amp;gt; b = tf.Variable(tf.zeros([3]), # same as numpy.zeros
name=&#39;b&#39;, trainable=False)
&amp;gt;&amp;gt;&amp;gt; x = tf.placeholder(tf.float32, name=&#39;x&#39;, shape=[None, 2]) # defining a placeholder named x
&amp;gt;&amp;gt;&amp;gt; # of type float32 and shape [None ,2] (None, i.e. any number of rows)
&amp;gt;&amp;gt;&amp;gt; y = tf.add(tf.matmul(x,W), b) # operation x.W + b
&amp;gt;&amp;gt;&amp;gt; s = tf.Session() # session created
&amp;gt;&amp;gt;&amp;gt; s.run(tf.initialize_all_variables()) # To initialize all the variables present in the current session
&amp;gt;&amp;gt;&amp;gt; s.run(y,feed_dict={x:[[1., 2.]]} # placeholder x feeded with value
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Operations&lt;/strong&gt;: Operations in tensoflow are functions which applies some transformations to tensors on the computational graph. Like Variables and placeholders tensorflow operations can also be named for easy identification in the computational graph.An operation may consist of multiple kernels, for different types of devices.For example,  an operation may have seperate CPU and GPU kernels so that it can be excuted more efficiently on GPU.In the previous example &lt;strong&gt;tf.matmul()&lt;/strong&gt; and &lt;strong&gt;tf.add()&lt;/strong&gt; are two operations.As it relevant from the above example , tensorflow operations can be nested.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Sessions&lt;/strong&gt;: A tensorflow session is responsible for interacting with the computational graph thus created and make necessary arrangements for execution of the graph.It allocates resources required by the computational graph and holds the values of variables.Tensorflow session object(say sess) is created using the &lt;strong&gt;tf.Session()&lt;/strong&gt; class.Finally we can the computational graph or a subpart of it using the &lt;strong&gt;sess.run()&lt;/strong&gt; method.All sess.run does is identify all the dependencies that compose the relevant subgraph, ensure that all the placeholder variables in the concerned subgraph are filled using feed_dict,and then start executing the subgraph.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
